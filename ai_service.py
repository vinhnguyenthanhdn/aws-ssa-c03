import streamlit as st
import json
from pathlib import Path

# Configure Gemini Keys
API_KEYS = []
if "GOOGLE_API_KEYS" in st.secrets:
    API_KEYS = [k.strip() for k in st.secrets["GOOGLE_API_KEYS"].split(",")]
elif "GOOGLE_API_KEY" in st.secrets:
    API_KEYS = [st.secrets["GOOGLE_API_KEY"]]

CACHE_FILE = Path(__file__).parent / "ai_cache.json"

def configure_genai():
    """Configure Google Generative AI with current API key."""
    import google.generativeai as genai
    if not API_KEYS: 
        return False
    
    # Ensure key index exists
    if "api_key_index" not in st.session_state:
        st.session_state.api_key_index = 0
    
    current_key = API_KEYS[st.session_state.api_key_index % len(API_KEYS)]
    genai.configure(api_key=current_key)
    return True

def rotate_key():
    """Rotate to next API key if rate limited."""
    if not API_KEYS: 
        return
    st.session_state.api_key_index = (st.session_state.api_key_index + 1) % len(API_KEYS)
    configure_genai()

def load_cache():
    """Load AI response cache from disk."""
    if not CACHE_FILE.exists():
        return {"explanations": {}, "theories": {}}
    try:
        return json.loads(CACHE_FILE.read_text(encoding='utf-8'))
    except:
        return {"explanations": {}, "theories": {}}

def save_cache(data):
    """Save AI response cache to disk."""
    try:
        CACHE_FILE.write_text(json.dumps(data, ensure_ascii=False, indent=2), encoding='utf-8')
    except Exception as e:
        print(f"Error saving cache: {e}")

def get_cached_content(category, key):
    """Retrieve cached AI response."""
    data = load_cache()
    return data.get(category, {}).get(key)

def save_cached_content(category, key, value):
    """Save AI response to cache."""
    data = load_cache()
    if category not in data: 
        data[category] = {}
    data[category][key] = value
    save_cache(data)

def get_ai_explanation(question, options, correct_answer, question_id):
    """Get AI explanation for a question answer."""
    # Check cache first
    cached = get_cached_content("explanations", question_id)
    if cached: 
        return cached

    max_retries = min(len(API_KEYS) + 2, 6)  # Try shifting keys first
    for attempt in range(max_retries):
        try:
            configure_genai()  # Ensure current key is set
            import google.generativeai as genai
            model = genai.GenerativeModel('gemini-1.5-flash')
            prompt = f"""
            B·∫°n l√† chuy√™n gia AWS SAA-C03. Nhi·ªám v·ª• c·ªßa b·∫°n l√† ph√¢n t√≠ch c√¢u h·ªèi tr·∫Øc nghi·ªám n√†y ƒë·ªÉ gi·∫£i th√≠ch cho h·ªçc vi√™n.
    
            **C√¢u h·ªèi:**
            {question}
    
            **C√°c l·ª±a ch·ªçn:**
            {options}
    
            **ƒê√°p √°n ƒë√∫ng:** {correct_answer}
    
            **Y√™u c·∫ßu Output (R·∫•t quan tr·ªçng):**
            - **TUY·ªÜT ƒê·ªêI KH√îNG** c√≥ l·ªùi ch√†o m·ªü ƒë·∫ßu (VD: "Ch√†o b·∫°n", "T√¥i l√† chuy√™n gia...").
            - **TUY·ªÜT ƒê·ªêI KH√îNG** c√≥ l·ªùi ch√∫c hay k·∫øt lu·∫≠n x√£ giao ·ªü cu·ªëi (VD: "Ch√∫c thi t·ªët", "Hy v·ªçng gi√∫p √≠ch...").
            - Ch·ªâ t·∫≠p trung v√†o n·ªôi dung chuy√™n m√¥n c√¥ ƒë·ªçng.
    
            **C·∫•u tr√∫c ph√¢n t√≠ch:**
            1. **üéØ Ph√¢n t√≠ch Y√™u c·∫ßu:** X√°c ƒë·ªãnh t·ª´ kh√≥a (keywords) v√† m·ª•c ti√™u c·ªßa ƒë·ªÅ b√†i.
            2. **‚úÖ Gi·∫£i th√≠ch ƒë√°p √°n ƒë√∫ng:** T·∫°i sao n√≥ ƒë√°p ·ª©ng t·ªët nh·∫•t y√™u c·∫ßu (v·ªÅ k·ªπ thu·∫≠t, chi ph√≠, best practice)?
            3. **‚ùå Gi·∫£i th√≠ch ƒë√°p √°n sai:** L√≠ do t·ª´ng ƒë√°p √°n c√≤n l·∫°i kh√¥ng ph√π h·ª£p.
            4. **üí° M·∫πo nh·ªõ nhanh:** Mapping t·ª´ kh√≥a <-> D·ªãch v·ª•.
            """
            response = model.generate_content(prompt)
            text = response.text
            # Save to cache
            save_cached_content("explanations", question_id, text)
            return text
        except Exception as e:
            if "429" in str(e):
                # Rotate key and retry
                rotate_key()
                continue 
            return f"‚ö† Kh√¥ng th·ªÉ t·∫£i ph√¢n t√≠ch t·ª´ AI. L·ªói: {str(e)}"
    
    return "‚ö† Kh√¥ng th·ªÉ t·∫£i ph√¢n t√≠ch t·ª´ AI sau nhi·ªÅu l·∫ßn th·ª≠."

def get_ai_theory(question, options, question_id):
    """Get AI theory explanation for AWS concepts in question."""
    # Check cache first
    cached = get_cached_content("theories", question_id)
    if cached: 
        return cached

    max_retries = min(len(API_KEYS) + 2, 6)
    for attempt in range(max_retries):
        try:
            configure_genai()
            import google.generativeai as genai
            model = genai.GenerativeModel('gemini-1.5-flash')
            prompt = f"""
            B·∫°n l√† t·ª´ ƒëi·ªÉn s·ªëng v·ªÅ AWS. H√£y gi·∫£i th√≠ch ng·∫Øn g·ªçn c√°c **D·ªãch v·ª•** ho·∫∑c **Kh√°i ni·ªám** AWS xu·∫•t hi·ªán trong vƒÉn b·∫£n sau:
    
            **Ng·ªØ c·∫£nh (C√¢u h·ªèi & ƒê√°p √°n):**
            {question}
            {options}
    
            **Y√™u c·∫ßu Output:**
            - Ch·ªâ t·∫≠p trung v√†o C√ÅC KH√ÅI NI·ªÜM/D·ªäCH V·ª§ (VD: AWS Lambda, IOPS, Consistency Model...).
            - V·ªõi m·ªói kh√°i ni·ªám: ƒê∆∞a ra ƒë·ªãnh nghƒ©a 1 d√≤ng v√† Use Case ch√≠nh 1 d√≤ng.
            - Kh√¥ng gi·∫£i th√≠ch c√¢u h·ªèi, kh√¥ng ph√¢n t√≠ch ƒë√∫ng sai.
            - Tr√¨nh b√†y d·∫°ng danh s√°ch Markdown s·∫°ch s·∫Ω.
            """
            response = model.generate_content(prompt)
            text = response.text
            # Save to cache
            save_cached_content("theories", question_id, text)
            return text
        except Exception as e:
            if "429" in str(e):
                rotate_key()
                continue
            return f"‚ö† L·ªói t·∫£i l√Ω thuy·∫øt: {str(e)}"
    
    return "‚ö† Kh√¥ng th·ªÉ t·∫£i l√Ω thuy·∫øt sau nhi·ªÅu l·∫ßn th·ª≠."

def init_ai_session_state():
    """Initialize AI-related session state."""
    if "api_key_index" not in st.session_state: 
        st.session_state.api_key_index = 0
    if "theories" not in st.session_state: 
        st.session_state.theories = {}
    if "explanations" not in st.session_state: 
        st.session_state.explanations = {}
